

> 在当今 AI 浪潮与大前端融合的时代，你是否也遇到了这些瓶颈？
>
> * **只会调 AI 接口？** 知道 GPT 很火，但只会调个 API。面对 LangChain、LangGraph 这样复杂的编排工具，不知道如何构建真正有用的 AI Agent 应用，无法实现复杂的业务逻辑。
>
> * **只会写“页面”？** 当项目需要你提供一套完整的后端服务时，面对 NestJS 这样的企业级框架，你对 IoC、AOP、数据库和鉴权一窍不通，只能依赖后端同事。
>
> * **还在用臃肿的 Electron？** 当大厂开始追求高性能、轻量级的桌面端时，你对基于 Rust 的 Tauri2 闻所未闻，只会用 Electron 打包，错失了性能优化中级奥秘。
>
> 如果你还在传统前端的“舒适圈”里打转，那么你的职业发展可能已经触及天花板。衡量一位优秀“AI全栈”或“大前端架构师”的关键标尺，早已不是精通某个框架，而是**打通前后端、驾驭 AI、并掌控跨端（Web/Desktop）应用交付的全链路能力**。



# **AI 大前端全栈架构师最佳技能树**



## 基础核心



无论浪潮如何变化，地基决定了你的上层建筑。架构师的深度，首先体现在基础的扎实程度。

* **计算机科学基础**:

  * 数据结构与算法：内功心法，决定你代码的效率和质量。

  * 计算机网络：(TCP/IP, HTTP/HTTPS, WebSocket)，理解 AI 应用中数据流转、流式响应的底层脉络。

  * 操作系统与数据库原理：支撑你构建高性能、高可用系统的理论基石。

* **编程语言**:

  * **TypeScript (精通)**: 这不再是可选项，而是**贯穿全栈的“普通话”**。从前端 UI、后端 API 到 AI 服务的编排，你需要深度掌握其类型系统、泛型、装饰器等高级特性。

  * **Python (熟悉)**: AI/ML 领域的“母语”。虽然本课程以 TS 为主，但能看懂 Python 实现，是你理解 AI 生态、阅读模型文档、与算法工程师高效协作的前提。

  * **SQL (熟练)**: 无论 AI 如何发展，数据始终是核心。你需要熟练掌握数据查询、关联和管理。



## 前端技术栈



前端是 AI 能力的“脸面”，是用户感知的最终出口。AI 应用的体验好坏，前端的选型至关重要。

* **框架与库**:

  * **React (精通)**: 现代前端的基石。你需要洞悉 Hooks 的本质、Context 的局限、Concurrent Mode（并发模式）对未来 AI 交互的意义。

  * **Tauri2 (熟练)**: **AI 落地桌面的“新王牌”**。相比 Electron，它更轻量、更安全、性能更优。掌握它，是你在 AI 应用落地场景中拉开差距的关键。

* **UI 与组件库**:

  * **shadcn/ui**: 它代表了新一代组件库的设计思想：**组合而非封装**。你不再是组件库的“使用者”，而是“拥有者”，这带来了前所未有的灵活性和可定制性。

  * **Tailwind CSS**: 原子化 CSS 的事实标准，极大提升 UI 构建效率和设计一致性。

* **工程化与构建**:

  * Vite / Rspack: 极致的开发体验与高性能构建，理解其底层的 ES Module 和 Native 编译原理。

  * pnpm: 高效的包管理工具，Monorepo 架构的最佳拍档。

* **状态管理**:

  * Zustand / Jotai: 掌握至少一种轻量级、原子化的状态管理方案，应对复杂 AI 应用中的瞬时状态和全局状态。



## 后端技术栈



后端是 AI 应用的服务化关键，负责复杂的业务逻辑、海量数据管理以及 AI 模型的调度。

* **框架**:

  * **NestJS (精通)**: Node.js 企业级开发的“答案”。它基于 TypeScript，完美融合了 OOP（面向对象）、FP（函数式）和 FRP（函数响应式）思想。其\*\*模块化、依赖注入（DI）、AOP（面向切面）\*\*的设计，是构建大型、可维护 AI 服务中台的理想选择。

* **数据库**:

  * **PostgreSQL (熟练)**: 功能最强大的开源对象-关系数据库。它的稳定性、可扩展性，以及对 JSON 的原生支持，使其成为业务数据的首选。

  * ORM: TypeORM 或 Prisma，掌握一种，精通其与 NestJS 的集成。

* **API 设计**:

  * RESTful API / WebSocket: 掌握传统请求与 AI **流式响应**的混合设计。

  * GraphQL (可选): 在数据查询复杂度极高的场景下有奇效。

* **身份验证与授权**:

  * JWT (JSON Web Tokens), OAuth 2.0: 架构师必备的安全防线。



## AI 应用层



这是连接“传统应用”与“AI 智能”的桥梁，是 **AI 全栈架构师的核心价值**所在。

* **核心框架**:

  * **LangChain.js (精通)**: AI 应用开发的“瑞士军刀”。你必须深入理解其六大核心抽象：

    * **Models**: 统一对接 LLMs (OpenAI, Anthropic, 本地模型) 的 I/O 接口。

    * **Prompts**: 可复用、可编程的提示词工程。

    * **Chains**: **“链式调用”**，将多个组件串联，完成单一、固定的任务流。

    * **Agents**: **“智能代理”**，让 LLM 具备“思考”和“使用工具 (Tools)”的能力，自主决策执行复杂工作流。

    * **Memory**: 记忆模块，让应用具备上下文感知能力，实现多轮对话。

    * **RAG (检索增强生成)**: AI 架构师的**必杀技**。结合外部知识库（向量数据库）生成答案，解决 LLM 的“幻觉”和“知识老旧”问题。

* **向量数据库 (Vector Databases)**:

  * 理解 Embeddings (向量嵌入) 的概念。

  * 掌握至少一种向量数据库的实现，如 `pgvector` (PostgreSQL 插件)、ChromaDB 或 Pinecone。

* **模型部署与使用**:

  * **OpenAI API**: 高效、安全地调用 GPT 系列模型。

  * **本地化部署 (熟悉)**: 了解如何使用 Ollama 等工具在本地运行 Llama 3, Mistral 等开源模型，这是实现数据隐私和降本增效的重要途径。



## 工程化与 DevOps



优秀的架构师不仅要“设计”系统，更要确保系统能被“高效、稳定”地开发、部署和运维。

* **代码管理**:

  * **Git (精通)**: 熟练掌握分支策略 (Git Flow, GitHub Flow), Code Review 规范。

* **Monorepo**:

  * **Turborepo**: **构建大型全栈项目的利器**。理解其“增量构建”、“任务缓存”和“任务编排”的核心优势，熟练使用 `pnpm workspaces` 组织项目。

* **CI/CD**:

  * GitHub Actions / GitLab CI: 自动化测试、构建和部署流程。

* **容器化与编排**:

  * **Docker (熟练)**: 应用容器化，保证开发、测试、生产环境的一致性。

  * **Kubernetes (了解)**: 容器编排，大规模部署时的必备技能。




# **AI 应用开发核心盘点**



我们的技术选型遵循以下几个核心原则：

1. **TypeScript First**: 全栈统一语言，最大化类型安全，消除前后端的数据契约“鸿沟”。

2. **现代化与生产力**: 拥抱最新的技术趋势（Tauri, shadcn/ui, Vite），利用高效工具链碾压传统开发体验。

3. **AI 整合能力**: 技术栈必须能与 AI 模型、向量数据库等生态无缝集成。

4. **可扩展与可维护**: 架构设计必须支撑企业级的长期演进和维护。



## 前端：React + Tauri + shadcn/ui



前端是 AI 功能的“展示层”，一个现代化、高性能、可定制的 UI 是留住用户的关键。

* **React**: 前端开发的“基石”。其庞大的生态、组件化思想和声明式 UI 理念，是构建复杂高交互性 AI 应用（如实时聊天、数据看板）的不二之选。

* **Tauri**: **为什么放弃 Electron？**

  * **痛点**: Electron 将 Node.js 和 Chromium 完整打包，导致应用体积臃肿（动辄 100MB+）、内存占用高、安全风险大。

  * **解决方案**: Tauri 使用 Rust 作为后端，前端直接渲染到系统原生的 WebView（如 Windows 的 WebView2, macOS 的 WKWebView）。这带来了**极致的性能**（秒级启动）、**极小的体积**（几 MB）和**更高的安全性**。对于需要访问本地文件、调用系统能力的 AI 助手类应用，Tauri 是降维打击。

* **shadcn/ui & Tailwind CSS**: **为什么告别传统组件库？**

  * **痛点**: 传统组件库（如 Ant Design, Element UI）封装过重，定制样式需要层层覆盖，极其痛苦，且难以实现品牌化的设计。

  * **解决方案**: shadcn/ui 带来了\*\*“组合式”\*\*的新范式。它不是库，而是一系列你可以“复制粘贴”到项目中的组件代码。你对组件拥有 100% 的控制权，结合 Tailwind CSS 的原子化类，可以极速构建出高度定制化、符合现代美学的 UI。



## 服务端：NestJS + PostgreSQL + pgvector



* **NestJS**: **为什么放弃 Express/Koa？**

  * **痛点**: Express/Koa 过于灵活，缺乏架构约束，导致大型项目后期代码混乱、难以维护和测试。

  * **解决方案**: NestJS 提供了**企业级的“骨架”**。它内置了依赖注入（DI）、模块化、AOP 等设计模式，强制开发者使用清晰的（Module, Controller, Service）分层架构。对于需要长期迭代、逻辑复杂的 AI 服务中台而言，NestJS 的规范性、可测试性和可维护性是无价的。

* **PostgreSQL + pgvector**: **AI 时代的“黄金搭档”**。

  * **痛点**: 构建 RAG（检索增强生成）应用，传统方案是“业务数据存 MySQL/Postgres，向量数据存 Pinecone/ChromaDB”，需要维护**两套异构数据库**，架构复杂、数据一致性难保证、成本高昂。

  * **解决方案**: `pgvector` 是 PostgreSQL 的一个扩展，让“地球上最先进的开源关系型数据库”**原生支持向量存储和检索**。

    * **架构简化**: 一套数据库，同时搞定业务数据（用户、订单）和 AI 数据（Embeddings），运维复杂度**骤降**。

    * **数据一致性**: 可以在**同一个事务**中同时操作业务数据和向量数据，保证绝对一致。

    * **成本效益**: 减少了需要运维的服务数量，极大降低了中小企业落地 AI 的成本。



## AI 应用层：LangChain.js



如果说 LLM 是 AI 应用的“发动机”，那么 LangChain.js 就是这个发动机的“变速箱、传动轴和控制单元”。

* **为什么不直接调用 OpenAI API？**

  * **痛点**: 直接调用 API，你只能做简单的“一问一答”。如果想实现“先上网搜索资料，再总结答案”、“先查询数据库，再生成报告”，或者“让 AI 记住我们的对话历史”，你需要手写大量的“胶水代码”来编排和管理这个复杂流程。

  * **解决方案**: LangChain.js 是一个\*\*“LLM 应用开发框架”**，它将这些复杂的编排工作抽象化、标准化。它提供了标准化的**“链 (Chains)”**来串联多步操作，提供了强大的**“代理 (Agents)”\*\*来让 AI 自主决策和使用工具。它帮你解决了 80% 的脏活累活，让你能专注于 20% 的核心业务逻辑。



## 工程化：Monorepo (pnpm workspace)



本项目采用 `pnpm workspace` 实现的 Monorepo 架构，将前端、后端等所有代码放在一个仓库中。

* **为什么不用传统 Polyrepo（多仓库）？**

  * **痛点**: 在全栈项目中，前后端分离在不同仓库，最大的痛点是\*\*“代码共享”**和**“数据契约”\*\*。例如，一个 API 的 TypeScript 类型定义，后端改了，前端如何第一时间同步？通常是发布一个私有的 npm 包，流程繁琐且低效。

  * **解决方案**: Monorepo 允许我们轻松地在前后端之间共享代码（例如 `packages/types`）。借助 `pnpm workspace`，这种共享是**瞬时**的，修改后立刻生效，保证了前后端的数据契约永不“失联”。





# **monorepo 工程化架构与面试重难点**

## 什么是 Monorepo？



Monorepo 是一种将多个逻辑上独立的项目（例如前端应用、后端服务、共享工具库）存储在**同一个代码仓库**（Repository）中的策略。与之相对的是 Polyrepo（或 Multi-repo），即每个项目都有自己独立的代码仓库。

在我们的课程项目中，你将看到一个清晰的 Monorepo 结构：



## Monorepo 的四大核心优势（“痛点”与“解决方案”）



采用 Monorepo 架构不是为了“炫技”，而是为了解决 Polyrepo 架构下的四大“顽疾”。



### 代码复用与共享



* **传统痛点**: 在 Polyrepo 中，前后端需要共享一段通用逻辑（例如，API 的类型定义、数据校验规则）。你必须将其抽成一个独立的 npm 包，搭建发布流程，推送到私有 npm 仓库。后端修改后，前端还需要 `npm update`... **整个链路繁琐、低效且易出错**。

* **Monorepo 解决方案**: **“无发布共享”**。我们只需创建一个 `packages/types` 目录，存放共享代码。前端和后端项目通过 `workspace:` 协议（详见下文）直接引用它。修改共享代码后，所有依赖它的项目**立即感知到变化**，开发效率呈指数级提升。



### 简化的依赖管理



* **传统痛点**: 在 Polyrepo 中，10 个项目就有 10 个 `node_modules`。项目 A 依赖 React 17，项目 B 依赖 React 18，导致版本不一致、兼容性踩坑、磁盘空间巨大浪费。

* **Monorepo 解决方案**: **“依赖提升” (Hoisting)**。通过 `pnpm`，可以将所有项目的公共依赖提升（hoist）到根目录的 `node_modules` 中。所有子项目共享同一个依赖实例，**彻底保证了版本的一致性**，并极大节省了磁盘空间。



### 原子化提交/原子化重构



* **传统痛点**: 一个新功能需要同时修改前后端。在 Polyrepo 中，你必须在两个仓库分别提交 PR (Pull Request)。如果后端 PR 合并了，前端 PR 被拒绝了，整个系统将处于一个\*\*“中间态”的薛定谔状态\*\*，极易引发线上故障。

* **Monorepo 解决方案**: **“单一事务”**。所有相关的修改都在一个仓库中。一个功能的实现，无论涉及多少个子项目，都可以通过**一次 `git commit`** 完成。代码历史清晰明了，回滚和 Code Review 变得极其简单。同理，你可以安全地进行跨项目的重构（例如修改一个被前后端同时使用的函数名）。



### 统一的开发体验与 CI/CD



* **传统痛点**: 10 个项目，你需要 `cd` 到 10 个目录，运行 10 次 `npm install`，配置 10 套 lint, test, build 脚本，以及 10 条 CI/CD 流水线。

* **Monorepo 解决方案**: **“根目录掌控一切”**。你可以在根目录定义统一的脚本，例如 `pnpm test` 就可以一次性跑完所有子项目的测试。CI/CD 流程也得以极大简化，一次代码推送就可以触发所有相关项目的构建、测试和部署。



## pnpm workspace 实战



`pnpm` 是一个快速、节省磁盘空间的包管理工具，它对 Monorepo (在 pnpm 中称为 workspace) 提供了最优雅的原生支持。



### `pnpm-workspace.yaml`





### 安装依赖



**告别 `cd`**！只需在项目根目录运行一次 `pnpm install`，pnpm 就会自动安装所有子项目（apps, packages）的依赖。



### 项目间引用（`workspace:` 协议）



这是 Monorepo 的“魔法”所在。假设我们的后端 `server-nestjs` 需要引用共享的 `packages/types`。

在 `apps/server-nestjs/package.json` 中这样声明：

`workspace:*` 协议是关键。它告诉 pnpm：“不要去 npm registry 下载这个包，它就在我们自己家里（工作区）”。pnpm 会通过符号链接（symlink）的方式，将 `packages/types` 链接到 `server-nestjs` 的 `node_modules` 中。



### 运行脚本（`--filter` 精准执行）



`pnpm` 提供了强大的 `--filter` 标志来在特定项目上执行命令。

* `pnpm -r dev`: （-r, recursive）递归地在**所有**子项目中运行 `dev` 脚本。

* `pnpm --filter client-tauri dev`: **只在** `client-tauri` 项目中运行 `dev` 脚本。

* `pnpm --filter "./apps/*" build`: 在 `apps` 目录下的**所有**项目中运行 `build` 脚本。

* `pnpm --filter "...@ai-project/types" test`: 运行 `types` 包以及**所有依赖** `types` 包的项目的 `test` 脚本（...是“依赖我”的意思）。



## 面试重难点（高薪必问）



当面试官问到 Monorepo，他想考察的是你的工程化深度和大型项目经验。

**Q1: 什么是 Monorepo？它和 Polyrepo 相比有什么优缺点？**

* **A:** （参考上文）清晰地解释定义。然后，系统地阐述 Monorepo 的四大优点（代码共享、依赖简化、原子提交、统一流程）。同时，你必须客观地提到它的缺点：

  * **构建/测试性能**: 仓库变大，一次性构建和测试所有项目会很慢。（**解决方案**: 这就是 `Turborepo` 或 `Nx` 这类构建系统的价值所在，它们能实现“增量构建”和“缓存”，只构建变化过的代码）。

  * **权限控制**: 权限控制更复杂，很难做到只让某个开发者访问特定的子项目。

  * **Git 性能**: 如果包含大量二进制文件或历史悠久，Git 仓库可能会变大。

**Q2: 你们项目为什么选择 Monorepo？它帮你们解决了什么具体问题？**

* **A:** (**黄金回答范本**)

> - “在我们这个 AI 全栈项目中，我们采用 Monorepo 主要是为了解决前后端代码共享和数据契约一致性的核心痛点。例如，我们用 packages/types 来统一定义 API 的请求/响应体、AI 模型的 Schema 定义、以及 Zod 校验规则。
>
> - 借助 pnpm workspace: 协议，我们的 server-nestjs 后端和 client-tauri 前端可以零成本地共享这些类型定义。当后端修改了 API 接口，前端的 TypeScript 会立即感知到类型错误，这在编译阶段就杜绝了大量潜在的 bug。
>
> - 如果用传统的 Polyrepo，我们必须把 types 发布成一个私有 npm 包，这个流程非常繁琐和低效。而 Monorepo 真正实现了\*\*‘一次定义，全栈共享’\*\*，极大地提升了我们的开发效率和代码质量。”

**Q3: 你知道哪些 Monorepo 的管理工具？它们有什么区别？**

* **A:**

  * **包管理器 (Workspace)**: `pnpm workspace`, `yarn workspace`。它们是“地基”，提供了基础的工作区管理能力，如依赖提升、项目间链接。我首选 `pnpm`，因为它通过其非扁平化的 `node_modules` 结构，能从根本上避免“幻影依赖”问题。

  * **构建系统 (Build System)**: `Turborepo` (Vercel 出品), `Nx` (Nrwl 出品)。它们是“上层建筑”，专注于解决 Monorepo 的**性能问题**。它们的核心是**构建缓存**和**任务编排**（智能分析任务依赖，并行执行）。

  * **Lerna**: 一个老牌工具，但现在其许多核心功能已被包管理器原生支持。现在 Lerna 官方也推荐和 Nx 结合使用。

**Q4: Monorepo 中如何管理依赖版本？如果两个项目需要同一个依赖的不同版本怎么办？**

* **A:**

> - “在 Monorepo 中，最佳实践是尽量统一所有子项目的核心依赖版本（如 React, NestJS），并将它们提升到根 package.json 中进行管理。
>
> - 不过，如果我们真的遇到了必须使用不同版本的情况（例如老项目A 依赖 lodash@3，新项目B 依赖 lodash@4），pnpm 在这方面比 npm 或 yarn v1 具有天然优势。
>
> - pnpm 的 node\_modules 结构是非扁平化的。每个包的 node\_modules 只包含其直接声明的依赖（通过符号链接）。因此，项目 A 和项目 B 可以完美共存各自的 lodash 版本，而不会像 npm v2/v3 那样产生版本冲突或覆盖。但这通常被视为一种需要避免的例外情况，而不是常规操作。”



# 前端开发：基于 Tauri 和 Shadcn/ui 打造高性能 AI 桌面应用



## 拥抱下一代桌面端方案：Tauri



### 为什么 AI 应用需要桌面端？为什么是 Tauri？



* **传统痛点**：Web 应用受限于浏览器沙箱，无法直接访问本地文件系统、调用底层硬件（如 GPU），且必须在线使用。对于需要处理本地知识库、注重数据隐私、追求极致性能的 AI 应用，Web 是一个“镣铐”。

* **Electron 的困境**：作为上一代方案，Electron 将整个 Node.js 和 Chromium 打包，导致应用体积臃肿（动辄 100MB+）、内存占用高、启动缓慢，这与 AI 应用追求轻快的目标背道而驰。

* **Tauri 解决方案**：\*\*“Rust 后端 + 系统 WebView”\*\*的革命性架构。

  * **极致性能与体积**：应用体积仅几 MB，内存占用极低，启动速度媲美原生应用。

  * **原生通信**：通过 Rust `Commands`，前端可以安全、高效地调用任何系统原生能力，这是实现复杂 AI 功能（如本地文件索引、离线模型推理）的基础。

  * **安全性**：Rust 的内存安全特性从根本上杜绝了许多安全漏洞。



### Tauri 核心实战



1. **项目初始化与集成**：从零开始，将 Tauri 无缝集成到现有的 React + Vite 项目中。

2. **核心通信机制**：精通前端 `invoke` 调用 Rust `Command` 的异步通信模式，并处理返回结果。

3. **系统原生能力调用**：实战窗口自定义、系统托盘菜单、原生文件对话框、系统通知等，打造沉浸式桌面体验。



## 高效构建现代化 UI 界面



### 告别“组件库”，拥抱“UI 工程”



* **传统痛点**：Ant Design 等传统组件库封装过重，样式定制困难，难以满足 AI 应用前卫、简洁的设计需求。

* **shadcn/ui 新范式**：它不是一个“库”，而是你的“私人组件代码库”。

  * **完全所有权**：通过 CLI 将组件代码直接复制到你的项目中，你可以 100% 掌控其样式和行为，不再受制于库的更新和限制。

  * **组合与定制**：基于 Radix UI（保证了无障碍访问性）和 Tailwind CSS，你可以像搭乐高一样组合和定制组件，极速构建出独一无二的 UI。



### UI 界面实战



1. **界面搭建**：从零构建一个优雅、响应式的 AI 聊天界面和知识库文件管理界面。

2. **表单处理与校验**：结合 `react-hook-form` 和 `zod`，实现健壮、类型安全的表单功能（如 API Key 设置、模型参数配置）。

3. **状态管理**：选用轻量级状态管理库 `Zustand`，以最少的模板代码，高效管理应用的全局状态（如对话历史、加载状态）。



## 前后端交互联调



1. **API 请求封装**：使用 `axios` 或 `fetch` 封装统一的 API 请求模块，处理请求、响应和错误。

2. **实时流式通信**：**AI 对话体验的灵魂**。使用 `WebSocket` 与 NestJS 服务端建立长连接，实时接收和渲染 LLM 生成的流式数据（token stream），打造“打字机”般的实时交互效果。



# 服务端开发：基于 NestJS 的企业级 AI 服务中台

## 奠定基石：NestJS 核心概念与企业级架构



### 为什么必须是 NestJS？



* **传统痛点**：Express/Koa 自由奔放，缺乏架构约束，项目规模一旦扩大，代码就会迅速腐化为难以维护的“面条代码”，尤其是在复杂的 AI 逻辑编排中，这简直是灾难。

* **NestJS 解决方案**：**“约定优于配置”**。NestJS 提供了清晰的、受控的架构“骨架”：

  * **模块化 (Module)**：将应用拆分为高内聚、低耦合的功能模块。

  * **依赖注入 (DI)**：解耦模块间的依赖关系，让代码变得极其容易测试和替换。

  * **分层架构 (Controller, Service)**：强制职责分离，Controller 处理 HTTP 请求，Service 封装核心业务逻辑，Repository 处理数据持久化。



### 架构实战



1. **数据库集成**：使用 `TypeORM` 连接 `PostgreSQL`，掌握实体（Entity）定义与数据迁移（Migration）的最佳实践。

2. **模块化设计**：根据业务领域（如用户、知识库、AI 对话），规划并创建应用的模块结构。



## 构建引擎：核心 API 接口开发



1. **AI 数据入口：文件上传与解析**：实现支持多种格式（PDF, TXT, Markdown）的文件上传接口，这是构建私有知识库的第一步。

2. **知识库管理**：开发标准的 `CRUD` 接口，用于管理知识库的生命周期。

3. **请求校验 (DTOs)**：使用 `class-validator` 和数据传输对象（DTOs）在请求入口就完成严格的数据校验，保证服务端的健壮性。

4. **安全防线：认证与授权**：使用 `Guards` 和 `JWT (JSON Web Tokens)` 实现用户认证与 API 授权，保护你的 AI 服务不被滥用。



## 业务核心实现：WebSocket 网关与流式响应



* **传统痛点**：使用普通 HTTP 轮询来获取 AI 结果，延迟高、体验差、服务器资源浪费严重。

* **WebSocket 解决方案**：**“一次连接，持续通信”**。

  1. **创建 WebSocket Gateway**：使用 NestJS 内置的 WebSocket 模块，轻松创建网关来处理客户端的连接、断开和消息事件。

  2. **实现消息推送**：将 Gateway 与 AI Service（下一章内容）解耦，实现从 AI Service 生成 token 后，能立即通过 Gateway 将其**实时推送**给指定的前端客户端。




# AI 应用开发：集成 LangChain.js 构建智能大脑

## 解锁 LLM：LangChain.js 框架入门



### 为什么需要 LangChain？



* **痛点**：直接调用 OpenAI API，你只能实现简单的“一问一答”，这远远不够。一个真正的 AI 应用需要：管理复杂的提示词、串联多个 API 调用、让 AI 能使用工具、记住对话历史……手写这些“胶水代码”极其复杂且容易出错。

* **LangChain 解决方案**：**“LLM 应用的乐高积木”**。LangChain 将构建 AI 应用的复杂流程，抽象为一个个标准化的、可插拔的“组件”，让你能像搭积木一样，快速、灵活地构建强大的 AI 应用。



### 核心实战



1. **六大核心模块概览**：快速理解 Models, Prompts, Chains, Indexes, Memory, Agents 的核心作用。

2. **对接你的第一个 LLM**：配置并成功调用 OpenAI API，完成“Hello, AI World!”。



## 核心技术揭秘：RAG（检索增强生成）全流程实战



### RAG：解决 LLM “幻觉”和“失忆”的银弹



* **痛点**：原生 LLM 存在两大致命缺陷：1. **知识老旧**（知识截止于训练日期）；2. **胡说八道**（对于不确定的问题，它会“一本正经地胡说八道”，即“幻觉”）。它也无法回答关于你私有文档（如公司内部 Wiki、项目 PDF）的问题。

* **RAG 解决方案**：**“先检索，再生成”**。我们不直接让 LLM 回答，而是分两步：

  1. **检索 (Retrieval)**：根据用户问题，从我们的私有知识库（向量数据库）中，**检索**出最相关的几段原文。

  2. **生成 (Generation)**：将用户问题和检索到的原文，**一起**作为上下文（Context）发给 LLM，并要求它“请基于我提供的这些资料来回答问题”。这样，LLM 的回答就被“锚定”在了事实依据上，从而极大减少幻觉，并能回答私域知识。



### RAG 全链路实战



#### **阶段一：数据索引（“喂”给 AI 知识）**



1. **`Document Loaders` (文档加载)**：从 PDF、TXT、Markdown 等不同来源加载原始数据。

2. **`Text Splitters` (文本分割)**：将长文档切分为更小的、语义完整的文本块（Chunks）。**（面试重点：分割策略直接影响检索效果）**

3. **`Embeddings` (文本向量化)**：调用 Embedding 模型（如 OpenAI 的 `text-embedding-ada-002`），将每个文本块转换为一个数学向量（Vector）。

4. **`Vector Stores` (向量存储)**：将文本块及其对应的向量存入 `pgvector` 数据库中。



#### **阶段二：检索与生成（让 AI “学以致用”）**



1. **构建 `RetrievalQA` 链**：这是 LangChain 中实现 RAG 的标准“链”。

2. **用户提问与向量检索**：当用户提问时，先将问题也转换为向量，然后在 `pgvector` 中进行相似性搜索，找出最匹配的文档块。

3. **`PromptTemplate` (提示词模板)**：精心设计提示词模板，指导 LLM 如何结合上下文来回答问题。

4. **实现流式输出 (Streaming)**：将 LLM 生成的 token 实时传递给服务端的 WebSocket Gateway，最终呈现给用户。



## 能力注入：将 AI 与 NestJS 服务融合



1. **封装 `AIService`**：将完整的 RAG 链路逻辑封装在一个独立的、可复用的 `AIService` 中，实现 AI 能力与业务逻辑的解耦。

2. **服务打通**：在 Controller 和 WebSocket Gateway 中注入 `AIService`，将前端的请求最终导向我们构建的 AI 智能大脑，完成整个应用的全链路闭环。


